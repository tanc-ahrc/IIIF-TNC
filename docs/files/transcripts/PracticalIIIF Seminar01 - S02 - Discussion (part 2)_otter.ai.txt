Joe Padfield  0:01  
To align historic maps. This is quite an active group within the IIIF community looking at working with maps. So be quite interesting to see what the correlation there is. Does anyone able to answer that one? Rob?

Rob Erdmann  0:21  
I was gonna say that it's my perception that the transformations we would usually use in the context of imaging, the paintings, let's say, are simpler than old maps for example. Because old maps have lots of weird distortions and they're often not metrically correct and except very approximately. So, with a good, well calibrated lens and everything else, then mostly just tomography is which is to say, the ways that images are worked when you move the camera, are, are enough, and probably not enough in the math context I would guess.

Joe Padfield  0:59  
All right, thank you very much. We have one other question. Let's see: Deep learning techniques exists for predicting fluorescent stains from bright field images in cellular microscopy,  (This will be obviously medical imaging people), but two modalities do not always have obvious shared features so training the dnn with fluoresecent images that are not well registered to their brightfield counterparts can lead to errors, what would be the best way to achieve better registration? I couldn't answer that question myself, if anyone else candidates. John any thoughts?

John Cupitt  1:37  
I'm very sorry I have not worked with fluoroscent microscopy, I couldn't really comment.

Joe Padfield  1:44  
 I think, I think that generally, it relates to the previous set of questions, is that how are you, when you know two images are supposed to have a correlation, but there are insufficient features between them, to actually compute it. I think, I think one, I can, sorry I can't remember who suggested it before, is the idea of artificially putting something in. So if there's a potential the ability to put in a physical object or a projected light, or anything that will mark consistency between the two modalities to give you that. But if you actually, if both images have no actual sort of comparison between the two I think registration is going to prove tricky. Okay. There are a few. There are a number of comments in the chats, a number of links, 

Anne McLaughlin  2:30  
Joe, I think we have a hand up, Adam Gibson,

Joe Padfield  2:33  
Yes, sorry.

Adam Gibson  2:37  
I think that as being the problem of training, a neural network or any other algorithm with training data that isn't perfect. And I think that is a well studied problem. I think there are sensitivity analyses, you can do, where you can you can understand the errors in the training data and look at how that can propagate to the errors in the final solution. Whatever the problem you're looking at is. I don't know anything about this specific problem here, but it wouldn't surprise me if related problems have been studied elsewhere. 

Joe Padfield  3:17  
Okay.

Anne McLaughlin  3:19  
We have a follow up question, from Kurt: saying if you're capturing your being just UV, and visible fluorescence, do you then use the same capture system eliminate the need for registration? I'm not trying to comment on that, just as a follow up.

Joe Padfield  3:36  
If you can ensure your camera doesn't move, and you're effectively changing the lighting environment, registration, would be easier. Rob?

Rob Erdmann  3:48  
Yeah I was just going to, to amplify what Ryan was saying about what you might call over registration, making two images that that really perfectly aligned when they shouldn't. This is a constant problem, and it's one of the reasons probably that it will be a long time before we have anything like a black box, because yeah, I mean from a machine learning standpoint you, you don't want to overfit. So we have regularisers. That is to say, we have a belief about how smooth the deformation field should be, so it shouldn't have rips and shouldn't have tears and it should have some kind of smoothness, but that's just completely up to the person sitting in front of the computer. I don't think there's a good theoretical guidance for how, how, how crunchy should the deformation field be because you can always basically beat it into shape so that they're perfectly aligned, but is it realistic? Probably not. And then you may be throwing out the very relevant information. For example, when you have imaging techniques that have different depths of penetration, then you expect them to be misregistered slightly because of the parallax effect of the third dimension. And that's a good thing and there's data there that you can use to infer things like scattering. So it's a really hard problem.

Joe Padfield  5:10  
Thank you, because I think, I think, I have heard people commenting about that difficulty when dealing with hyperspectral imaging. So effectively, your focus is changing across quite a wide wavelength difference that some actual processing of the individual slices within the data cube may be required. In addition to just a simple registration of the most feature rich slice to something else. But that's not something I have explored too much but, yes, so possibly other. Anyone like to comment on that at all, or difficulties? Ryan?

Ryan Baumann  5:49  
Just on the multispectral imaging aspect. A lot of times with different multispectral setups you do need to register, or may want to register, slices, you know, taken with the same system through the same camera because the different wavelengths will have different like penetrating depths, different like excitation volumes for fluorescence, as far as like, what portion of the object is getting light reflected through it or into it, or absorbed into it. And also you'll have different angles of diffraction or refraction on the on the lens or if you're using the filter wheel they can slightly shift the image some if the angle isn't like perfectly parallel to the lens and your filter wheel is coming through and slightly changing the, the image registration between, between those images so it's a, it's something that you think about with multimodal materials.

Joe Padfield  6:43  
Okay. Okay, thank you. Now, it is in the UK anyway. It's half past five. So we have a little bit more time. Do we have any more questions or points people in the panel would like to add? Do we have any more questions from the audience? If not, I've got a question for Rob. Now, okay, I'll ask my question, Rob. You mentioned in passing there that you're working on a web-based interface to the toolkits. When, effectively, a number of other people discussing the various bits of software they use, but you've given some extremely nice examples. So what I was wondering is how complex, would it be for other people to test it, and what sort of timeframe, roughly, are we talking years, months, days, minutes in rough areas?

Rob Erdmann  7:40  
Yeah, it's, it's always three years away. Yeah, it's, it's, it's a lot of pain. I mean, developing user interfaces is really a whole extra level of painful, of course, as Giles has experienced I think. Yeah, there's a saying in user interface design that it's impossible to make anything foolproof, because fools are so ingenius. And the basis of that is just that, as one who uses the software who's very deep into it, there's a whole other level of making it so that it's well understood, so that it's intuitive, so that it's robust. And, and this now involves multiple programming languages too, because now you have a JavaScript side of things, and then we're not going to be doing the computations, at least in my case, inside the browser, because I need a GPU, ideally, so this means then having a backend and a live connection between the two. So, yeah, maybe a couple years for a prototype, and if people would like to test it they're welcome to contact me, but I can't make any promises because like I said it's perpetually three years in the future. 

Joe Padfield  8:58  
This this is sort of off topic slightly. Have you considered the notion of setting it up as a service? So I'm involved in a number of EU projects related to the European Open Science Cloud and these sorts of things where potentially you can say, 'Oh, can I borrow some supercomputing for a few minutes and process my images?'. So, could you sort of see it being dealt with with that type of solution?

Rob Erdmann  9:21  
Oh yeah, yeah, definitely. I mean, the big the computation is not so bad anymore, in fact you can get, you know, free GPUs from Google without much trouble and so running the service wouldn't be so hard. The bandwidth is the is the main trouble. And insofar as uploading the images, you know, because these are, these are heavy, heavy images and so viewing them is no big deal because then your screen is basically the bottleneck. Now there's only 4 million pixels that you ever have to show at once, but to actually get these big images up into the cloud, so that you're doing all the heavy lifting, there is going to just going to be a pain so I'm not, I'm not quite sure what the right answer is there. Maybe internet speeds will become fast enough that it becomes feasible. Or maybe you'll just have to have your images, locally and then you could mount them from of the clouds somehow or something I don't really know. Maybe John has thoughts about this?

Joe Padfield  10:17  
 John,

John Cupitt  10:18  
I was only gonna say that, although I think you're right, you'll need a powerful GPU to derive the transports, once they've been derived, there's not very much computation left to do. And there it is feasible to do it in a browser.

Rob Erdmann  10:31  
Oh yeah, for sure, I agree.

John Cupitt  10:33  
Yeah, you can do things like fetching sections of the fence for on demand as well. So, which I've actually done in other projects, so I think you could probably, probably handle that.

Unknown Speaker  10:45  
Yeah, I mean my own viewer as has a homography ability, built in. So you can you can have an arbitrary homography between pairs of images. But the problem I was addressing was: I'm a user and I have a stack of visible and a stack of infrared and a stack of X-rays and I want them to all come together into a nice stack. How do I do that, that's, that's what I'm trying to solve with my software and and turning that into a cloud service has been hard, I think. But but it certainly, I do envision it within the Rijksmuseum for example, that, you know, I have a powerful computer under my desk and then my colleagues would have the URL to an internal link. And there's a shared file system behind the scenes we have a sort of research network so then someone can put their images on the shared file server, and then use the GPUs under my desk while they manipulate the images and ultimately view them through their own browser.

Joe Padfield  11:51  
Thank you. We've had a couple of extra questions. It's a question from Richard, which I'll come back to before we finish, but we've had one question about PT GUI, sort of translating this to be slightly more generic: So not speaking to John, and Rob here because, obviously, the answer is you write new software, How many people have had the opportunity to interact with software developers to say like PT GUI is great, but it doesn't do X, Y and Z? Are there any possibilities that PT GUI could be changed? Andrew, I think you've had your hand up before I started talking, but did you want to add to that, or

Andrew Bruce  12:34  
I just want to say that you know obviously, we've actually spoken a lot about incredibly complex transformations processes, developing software, you know, would I recommend using PT GUI as a kind of starter. Well yeah. Hugely, it's it's a really wonderful piece of software, which you can make 10 gigapixel files on a laptop, honestly. You know I have spoken to the developers, and you know we are an incredibly small, kind of market for them. And I guess their answer is that actually, you know, there are, I think about four steps that you can do, which will make it work very well for the kinds of mosaicking I do. But like I said you know it's not, it's definitely workarounds, and it means that we spend a lot of time kind of wrestling, to make a do what we want.

Joe Padfield  13:27  
Maria, did you want to add

Maria Villafane  13:31  
Actually it's more. Yeah, it's more like a ... like something that occured to me, talking about time, and it's nice and, how do you say, yes, on with the comment of Andrew just now that we are all talking about different ways of doing the registration, but one thing that occurred to me, because specifically is my, one of my concerns in my research, is basically I wanted to have like, a feeling of how long do you take to do your registrations? Either if that's manually or computationally? Like in an average, basically in general, this is an open question to all of the presenters. And for the audience to know, like, how, how long do you take to do the registration, from when you have your fixed moving image until you have the registration as you like it for working purposes?

Joe Padfield  14:31  
Less time that I did, and probably slightly more time than I should have done. I think, it's it's I'm not sure if anyone else wants to answer it's quite a tricky question is, well hey I haven't done registration for a little while, but it comes down to the purpose of the registration. So if the registration is for someone to have a quick look at something, then 5-10 minutes. If actually you're looking at a publication, or wanting to do any sort of, you know, actually any mathematical comparison, or looking at sort to try and infer more complex things behind it, then longer. I think, I say it's 'a how long is a piece of string' answer. And so that, I guess, the required time, hopefully is diminishing, and a number of the suggestions that have been put forward and different pieces of software will be decreasing that, so it should be minutes. I do remember spending hours on this type of thing in years gone by. But I would have thought he would be minutes if not less these days if possible. Anyone else want to comment on that? No, more comments? Okay. The last thing I was wanting to say, and people can we are planning to send emails around to people who registered to ask similar questions to this. But Richard has put forward the question is, what kind of forum stroke project is there to continue this discussion? Perhaps with an eye to standardisation and what can be done... As I said at the start, this, this seminar is kind of a side seminar to our IIIF project, and we are interested in how, particularly the discussion to do with recording the transformations and how they might be applied on the fly into the future IIIF standards or viewers. But that goes beyond the scope of our current project, but the IIIF consortium as a whole is quite a big, very friendly family. And I think the, it may well be some discussions about how transformation metrics can be applied in a IIIF process or how best to relate to the IIIF standard may well be able to continue within the IIIF consortium itself, potentially within a technical group, we may well be having other events as part of this project, or subsequent projects, and we will be asking what sorts of things people would need, and a few of the possibilities were further discussion, specific questions submitted and answers, reports, lists of software, with links to documentation. There's a number of different things that could be useful. I think we've had some examples of a number of different ways of doing registration here; from the simple sort of 'looking at it' backwards and forwards, to much more complex processes using AI, and other more computationally intensive approaches. So I think there are two issues here: there's one What can I do now? and what would I like to do in the future? And I think a number of people have mentioned ensuring that the quality of the images you capture in the first place is as high as realistic. At the time you're capturing them isn't always a good strong point to start with. And you may won't be able to apply better registration software in the future. But we are going to send these questions around to see what sorts of things people would like next or we could get out next. Some of which we might be able to help with, within our project, and some of which may well need to be handed on for other projects at the moment we're happy to receive questions we may not be able to answer them, and then pass them on. As another forum develops. But as I said, when it comes to the sort of IIIF standard, the actual IIIF community itself may well be a good place to have those discussions. There are larger entities, shall we say, within the heritage environment. One of them may well be ERIHS, which is their European Research Infrastructure heritage science, and it might be possible that that type of entity in the future might be able to act as a starting point, when people are trying to answer these sorts of questions, but it's not solved yet. But if people have suggestions of forums that we could let people know about, please do let us know and we can add them to our project website, and give people directions on where they might be able to go.

I would like to say, now we've reached the end of our time a very big thank you to all of the panellists for your answers and questions and discussions, and thank you very much for all of the participants. We still have a sizable number of participants right down to the bitter end. And thank you very much for continuing. I think we sort of peaked at over 150 attendees, which is, which is great. And so, yes. So again, thank you very much for everybody, and it's very great to hear you all. The videos will be processed and segmented, to a certain extent, not sure how segmented we can do, and they will be put up on the project website, the Twitter feed has been dropped into the chat, but effectively our Twitter handle is @practicalIIIF. And that's a good place to watch for when the new information is out. There as I said, we will email everyone who registered for this event to say when the videos are available. So, again, I would like to thank you. And if no one has any last comments they'd like to make, I think we are able to draw the meeting to a close. Okay, well thank you very much, and good night, good morning, good evening, whatever time zone people are at. Okay, bye bye everybody. Thank you very much. 


